{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX39ERUE9WQK",
        "outputId": "08bcde70-9e6c-4562-e648-efa25c8014d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import re\n",
        "import sys\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "#from utils import write_status\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "def preprocess_word(word):\n",
        "    # Remove punctuation\n",
        "    word = word.strip('\\'\"?!,.():;')\n",
        "    # Convert more than 2 letter repetitions to 2 letter\n",
        "    # funnnnny --> funny\n",
        "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
        "    # Remove - & '\n",
        "    word = re.sub(r'(-|\\')', '', word)\n",
        "    return word\n",
        "\n",
        "\n",
        "def is_valid_word(word):\n",
        "    # Check if word begins with an alphabet\n",
        "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
        "\n",
        "\n",
        "def handle_emojis(tweet):\n",
        "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
        "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
        "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
        "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
        "    # Love -- <3, :*\n",
        "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
        "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
        "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
        "    # Sad -- :-(, : (, :(, ):, )-:\n",
        "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
        "    # Cry -- :,(, :'(, :\"(\n",
        "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
        "    return tweet\n",
        "\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    processed_tweet = []\n",
        "    # Convert to lower case\n",
        "    tweet = tweet.lower()\n",
        "    # Replaces URLs with the word URL\n",
        "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
        "    # Replace @handle with the word USER_MENTION\n",
        "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
        "    # Replaces #hashtag with hashtag\n",
        "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
        "    # Remove RT (retweet)\n",
        "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
        "    # Replace 2+ dots with space\n",
        "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
        "    # Strip space, \" and ' from tweet\n",
        "    tweet = tweet.strip(' \"\\'')\n",
        "    # Replace emojis with either EMO_POS or EMO_NEG\n",
        "    tweet = handle_emojis(tweet)\n",
        "    # Replace multiple spaces with a single space\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "    words = tweet.split()\n",
        "\n",
        "    for word in words:\n",
        "        word = preprocess_word(word)\n",
        "        if is_valid_word(word):\n",
        "            if use_stemmer:\n",
        "                word = str(porter_stemmer.stem(word))\n",
        "            processed_tweet.append(word)\n",
        "\n",
        "    return ' '.join(processed_tweet)\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def preprocess_csv(csv_file_name, processed_file_name, test_file=False):\n",
        "    save_to_file = open(processed_file_name, 'w')\n",
        "\n",
        "    with open(csv_file_name, 'r',encoding = \"ISO-8859-1\") as csv:\n",
        "        #next(csv)\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            tweet_id = line[:line.find(',')]\n",
        "            if not test_file:\n",
        "                line = line[1 + line.find(','):]\n",
        "                positive = int(line[:line.find(',')])\n",
        "            line = line[1 + line.find(','):]\n",
        "            tweet = line\n",
        "            processed_tweet = preprocess_tweet(tweet)\n",
        "            if not test_file:\n",
        "                save_to_file.write('%s,%d,%s\\n' %\n",
        "                                   (tweet_id, positive, processed_tweet))\n",
        "            else:\n",
        "                save_to_file.write('%s,%s\\n' %\n",
        "                                   (tweet_id, processed_tweet))\n",
        "            #write_status(i + 1, total)\n",
        "    save_to_file.close()\n",
        "    print('\\nSaved processed tweets to: %s' % processed_file_name)\n",
        "    return processed_file_name\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) != 2:\n",
        "        print('Usage: python preprocess.py <raw-CSV>')\n",
        "        exit()\n",
        "    use_stemmer = False\n",
        "    csv_file_name = '/content/drive/My Drive/adm_project_dataset/dataset/test_stanford_ds.csv'\n",
        "    processed_file_name = csv_file_name[:-4] + '-processed.csv'\n",
        "    if use_stemmer:\n",
        "        porter_stemmer = PorterStemmer()\n",
        "        processed_file_name = csv_file_name[:-4] + '-processed-stemmed.csv'\n",
        "    preprocess_csv(csv_file_name, processed_file_name, test_file=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: python preprocess.py <raw-CSV>\n",
            "\n",
            "Saved processed tweets to: /content/drive/My Drive/adm_project_dataset/dataset/test_stanford_ds-processed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGBsxAUrJR6E",
        "outputId": "eeaeb9ce-0f4c-4961-8e7e-432a7a564ae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk import FreqDist\n",
        "import pickle\n",
        "import sys\n",
        "#from utils import write_status\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Takes in a preprocessed CSV file and gives statistics\n",
        "# Writes the frequency distribution of words and bigrams\n",
        "# to pickle files.\n",
        "\n",
        "\n",
        "def analyze_tweet(tweet):\n",
        "    result = {}\n",
        "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
        "    result['URLS'] = tweet.count('URL')\n",
        "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
        "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
        "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
        "        'URL', '')\n",
        "    words = tweet.split()\n",
        "    result['WORDS'] = len(words)\n",
        "    bigrams = get_bigrams(words)\n",
        "    result['BIGRAMS'] = len(bigrams)\n",
        "    return result, words, bigrams\n",
        "\n",
        "\n",
        "def get_bigrams(tweet_words):\n",
        "    bigrams = []\n",
        "    num_words = len(tweet_words)\n",
        "    for i in range(num_words - 1):\n",
        "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def get_bigram_freqdist(bigrams):\n",
        "    freq_dict = {}\n",
        "    for bigram in bigrams:\n",
        "        if freq_dict.get(bigram):\n",
        "            freq_dict[bigram] += 1\n",
        "        else:\n",
        "            freq_dict[bigram] = 1\n",
        "    counter = Counter(freq_dict)\n",
        "    return counter\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) != 2:\n",
        "        print('Usage: python stats.py <preprocessed-CSV>')\n",
        "        exit()\n",
        "    num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
        "    num_mentions, max_mentions = 0, 0\n",
        "    num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
        "    num_urls, max_urls = 0, 0\n",
        "    num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
        "    num_bigrams, num_unique_bigrams = 0, 0\n",
        "    all_words = []\n",
        "    all_bigrams = []\n",
        "    csv_file_name = 'train-processed.csv'\n",
        "    with open('/content/drive/My Drive/adm_project_dataset/dataset/train_stanford_ds-processed.csv', 'r') as csv:\n",
        "        next(csv)\n",
        "        lines = csv.readlines()\n",
        "        num_tweets = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            t_id, if_pos, tweet = line.strip().split(',')\n",
        "            if_pos = int(if_pos)\n",
        "            if if_pos:\n",
        "                num_pos_tweets += 1\n",
        "            else:\n",
        "                num_neg_tweets += 1\n",
        "            result, words, bigrams = analyze_tweet(tweet)\n",
        "            num_mentions += result['MENTIONS']\n",
        "            max_mentions = max(max_mentions, result['MENTIONS'])\n",
        "            num_pos_emojis += result['POS_EMOS']\n",
        "            num_neg_emojis += result['NEG_EMOS']\n",
        "            max_emojis = max(\n",
        "                max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
        "            num_urls += result['URLS']\n",
        "            max_urls = max(max_urls, result['URLS'])\n",
        "            num_words += result['WORDS']\n",
        "            min_words = min(min_words, result['WORDS'])\n",
        "            max_words = max(max_words, result['WORDS'])\n",
        "            all_words.extend(words)\n",
        "            num_bigrams += result['BIGRAMS']\n",
        "            all_bigrams.extend(bigrams)\n",
        "            #write_status(i + 1, num_tweets)\n",
        "    num_emojis = num_pos_emojis + num_neg_emojis\n",
        "    unique_words = list(set(all_words))\n",
        "    with open(csv_file_name[:-4] + '-unique.txt', 'w') as uwf:\n",
        "        uwf.write('\\n'.join(unique_words))\n",
        "    num_unique_words = len(unique_words)\n",
        "    num_unique_bigrams = len(set(all_bigrams))\n",
        "    print('\\nCalculating frequency distribution')\n",
        "    # Unigrams\n",
        "    freq_dist = FreqDist(all_words)\n",
        "    pkl_file_name = csv_file_name[:-4] + '-freqdist.pkl'\n",
        "    with open(pkl_file_name, 'wb') as pkl_file:\n",
        "        pickle.dump(freq_dist, pkl_file)\n",
        "    print('Saved uni-frequency distribution to %s' % pkl_file_name)\n",
        "    # Bigrams\n",
        "    bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
        "    bi_pkl_file_name = csv_file_name[:-4] + '-freqdist-bi.pkl'\n",
        "    with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
        "        pickle.dump(bigram_freq_dist, pkl_file)\n",
        "    print('Saved bi-frequency distribution to %s' % bi_pkl_file_name)\n",
        "    print('\\n[Analysis Statistics]')\n",
        "    print('Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets))\n",
        "    print('User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions))\n",
        "    print('URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls))\n",
        "    print('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))\n",
        "    print('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))\n",
        "    print('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: python stats.py <preprocessed-CSV>\n",
            "\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 1048575, Positive: 248576, Negative: 799999\n",
            "User Mentions => Total: 462669, Avg: 0.4412, Max: 12\n",
            "URLs => Total: 41192, Avg: 0.0393, Max: 4\n",
            "Emojis => Total: 8290, Positive: 6651, Negative: 1639, Avg: 0.0079, Max: 16\n",
            "Words => Total: 13196132, Unique: 208325, Avg: 12.5848, Max: 40, Min: 0\n",
            "Bigrams => Total: 12150180, Unique: 2363789, Avg: 11.5873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STI3TKjhUtQr",
        "outputId": "13704c96-ad6a-4421-c67f-bbf5ca2bb645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\n",
        "from sklearn import svm\n",
        "#import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Performs classification using SVM.\n",
        "\n",
        "FREQ_DIST_FILE = './train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = './train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = './train_stanford_ds-processed.csv'\n",
        "TEST_PROCESSED_FILE = './test_stanford_ds-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle\n",
        "    file which has a Counter object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of bigrams to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {bigram:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            #utils.write_status(i + 1, total)\n",
        "    print('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    \"\"\"Split the data into training and validation sets\n",
        "    Args:\n",
        "        tweets (list): list of tuples\n",
        "        validation_split (float, optional): validation split %\n",
        "    Returns:\n",
        "        (list, list): training-set, validation-set\n",
        "    \"\"\"\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print('Extracting features & training batches')\n",
        "    clf = svm.LinearSVC(C=0.1)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        #utils.write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print('\\n')\n",
        "    print('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            #utils.write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            #utils.write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'svm.csv')\n",
        "        print('\\nSaved to svm.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Extracting features & training batches\n",
            "\n",
            "\n",
            "Testing\n",
            "\n",
            "Correct: 90468/104858 = 86.2767 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAudAtYlWmHv",
        "outputId": "e831f2d6-a410-4c66-8e4a-3cb8f173fd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "#import utils\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using CNN.\n",
        "\n",
        "FREQ_DIST_FILE = './train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = './train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train_stanford_ds-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/test_stanford_ds-processed.csv'\n",
        "GLOVE_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/glove.twitter.27B.200d.txt'\n",
        "dim = 200\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    \"\"\"\n",
        "    Extracts glove vectors from seed file only for words present in vocab.\n",
        "    \"\"\"\n",
        "    print('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            #utils.write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    \"\"\"\n",
        "    Generates a feature vector for each tweet where each word is\n",
        "    represented by integer index based on rank in vocabulary.\n",
        "    \"\"\"\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"\n",
        "    Generates training X, y pairs.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            #utils.write_status(i + 1, total)\n",
        "    print('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #train = len(sys.argv) == 1\n",
        "    train=1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 128\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    # Create and embedding matrix\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    # Seed it with GloVe vectors\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(600))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "        filepath = \"/content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        history=model.fit(tweets, labels, batch_size=128, epochs=30, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "        plt.plot(history.history['acc'])\n",
        "        plt.plot(history.history['val_acc'])\n",
        "        plt.title('model accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'val'], loc='upper left')\n",
        "        plt.show()\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'val'], loc='upper left')\n",
        "        plt.show()\n",
        "    else:\n",
        "        model_save_name = 'classifier.pt'\n",
        "        model = load_model('/content/drive/My Drive/adm_project_dataset/dataset/models')\n",
        "        print(model.summary())\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'cnn.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking for GLOVE seeds\n",
            "\n",
            "\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Epoch 1/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.3478 - acc: 0.8536\n",
            "Epoch 00001: loss improved from inf to 0.34781, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-01-0.348-0.854-0.314-0.869.hdf5\n",
            "7373/7373 [==============================] - 1637s 222ms/step - loss: 0.3478 - acc: 0.8536 - val_loss: 0.3142 - val_acc: 0.8688\n",
            "Epoch 2/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.3133 - acc: 0.8700\n",
            "Epoch 00002: loss improved from 0.34781 to 0.31329, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-02-0.313-0.870-0.308-0.873.hdf5\n",
            "7373/7373 [==============================] - 1615s 219ms/step - loss: 0.3133 - acc: 0.8700 - val_loss: 0.3075 - val_acc: 0.8730\n",
            "Epoch 3/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.2973 - acc: 0.8776\n",
            "Epoch 00003: loss improved from 0.31329 to 0.29733, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-03-0.297-0.878-0.311-0.873.hdf5\n",
            "7373/7373 [==============================] - 1619s 220ms/step - loss: 0.2973 - acc: 0.8776 - val_loss: 0.3106 - val_acc: 0.8730\n",
            "Epoch 4/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.2847 - acc: 0.8837\n",
            "Epoch 00004: loss improved from 0.29733 to 0.28471, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-04-0.285-0.884-0.305-0.873.hdf5\n",
            "7373/7373 [==============================] - 1612s 219ms/step - loss: 0.2847 - acc: 0.8837 - val_loss: 0.3047 - val_acc: 0.8735\n",
            "Epoch 5/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.2731 - acc: 0.8892\n",
            "Epoch 00005: loss improved from 0.28471 to 0.27314, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-05-0.273-0.889-0.309-0.872.hdf5\n",
            "7373/7373 [==============================] - 1611s 219ms/step - loss: 0.2731 - acc: 0.8892 - val_loss: 0.3086 - val_acc: 0.8720\n",
            "Epoch 6/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.2621 - acc: 0.8943\n",
            "Epoch 00006: loss improved from 0.27314 to 0.26213, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-06-0.262-0.894-0.312-0.870.hdf5\n",
            "7373/7373 [==============================] - 1612s 219ms/step - loss: 0.2621 - acc: 0.8943 - val_loss: 0.3116 - val_acc: 0.8702\n",
            "Epoch 7/30\n",
            "7373/7373 [==============================] - ETA: 0s - loss: 0.2423 - acc: 0.9030\n",
            "Epoch 00007: loss improved from 0.26213 to 0.24227, saving model to /content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-07-0.242-0.903-0.328-0.870.hdf5\n",
            "7373/7373 [==============================] - 1610s 218ms/step - loss: 0.2423 - acc: 0.9030 - val_loss: 0.3279 - val_acc: 0.8699\n",
            "Epoch 8/30\n",
            "6987/7373 [===========================>..] - ETA: 1:23 - loss: 0.2333 - acc: 0.9065Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH5lK1YzDXCX",
        "outputId": "831d79bb-32a5-4240-a110-ba9fdf4fe53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import load_model, Model\n",
        "#import utils\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Extracts dense vector features from penultimate layer of CNN model.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train-processed-freqdist-bi.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train-processed-freqdist.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train_stanford_ds-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/test_stanford_ds-processed.csv'\n",
        "GLOVE_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/glove.twitter.27B.200d.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            #utils.write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            #write_status(i + 1, total)\n",
        "    print('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    path = \"/content/drive/My Drive/SAVED_MODEL_FILENAME_PB\"\n",
        "    #torch.save(model.state_dict(), path)\n",
        "    model = load_model('/content/drive/My Drive/adm_project_dataset/dataset/models/4cnn-01-0.345-0.854-0.312-0.869.hdf5')\n",
        "    model = Model(model.layers[0].input, model.layers[-3].output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "    test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "    predictions = model.predict(test_tweets, batch_size=1024, verbose=1)\n",
        "    np.save('test-feats.npy', predictions)\n",
        "    predictions = model.predict(tweets, batch_size=1024, verbose=1)\n",
        "    np.save('train-feats.npy', predictions)\n",
        "    np.savetxt('train-labels.txt', labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c45b62fec2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PROCESSED_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c45b62fec2ef>\u001b[0m in \u001b[0;36mtop_n_words\u001b[0;34m(pkl_file_name, N, shift)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOf\u001b[0m \u001b[0mform\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mfreq_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mmost_common\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train-processed-freqdist.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIEphT4RazXJ",
        "outputId": "f6e6155a-a5e1-4c4f-deee-a20d9a6884d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "import pickle\n",
        "#import utils\n",
        "\n",
        "# Performs SVM classification on features extracted from penultimate layer of CNN model.\n",
        "\n",
        "\n",
        "TRAIN_FEATURES_FILE = './train-feats.npy'\n",
        "TRAIN_LABELS_FILE = './train-labels.txt'\n",
        "TEST_FEATURES_FILE = './test-feats.npy'\n",
        "CLASSIFIER = 'SVM'\n",
        "MODEL_FILE = 'cnn-feats-%s.pkl' % CLASSIFIER\n",
        "TRAIN = True\n",
        "C = 1\n",
        "MAX_ITER = 1000\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "if TRAIN:\n",
        "    X_train = np.load(TRAIN_FEATURES_FILE)\n",
        "    y_train = loadtxt(TRAIN_LABELS_FILE, dtype=float).astype(int)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
        "\n",
        "    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "    if CLASSIFIER == 'SVM':\n",
        "        model = svm.LinearSVC(C=C, verbose=1, max_iter=MAX_ITER)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    print(model)\n",
        "    del X_train\n",
        "    del y_train\n",
        "    with open(MODEL_FILE, 'wb') as mf:\n",
        "        pickle.dump(model, mf)\n",
        "    val_preds = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, val_preds)\n",
        "    print(\"Val Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "else:\n",
        "    with open(MODEL_FILE, 'rb') as mf:\n",
        "        model = pickle.load(mf)\n",
        "    X_test = np.load(TEST_FEATURES_FILE)\n",
        "    print(X_test.shape)\n",
        "    test_preds = model.predict(X_test)\n",
        "    results = zip(map(str, range(X_test.shape[0])), test_preds)\n",
        "    save_results_to_csv(results, 'cnn-feats-svm-linear-%.2f-%d.csv' % (C, MAX_ITER))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(943718, 600) (943718,) (104858, 600) (104858,)\n",
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=1)\n",
            "Val Accuracy: 87.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoJdmu7HTvwE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating the dataset\n",
        "data = {'SVM':86.1785, 'CNN':89.29 , 'CNN+SVM':90.78 ,\n",
        "        'CNN max len=20':89.04}\n",
        "models = list(data.keys())\n",
        "accuracy = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (8, 5))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(models,accuracy, color ='blue',\n",
        "        width = 0.4)\n",
        "\n",
        "plt.xlabel(\"Approaches\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Comparisons of differnet approaches based on accuracy on validation data\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qomj1N429K3e"
      },
      "source": [
        "LSTM- Stanford"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM-tU7b-SQ1A",
        "outputId": "643082ff-fdd8-475e-d089-fa5d31039955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import LSTM\n",
        "#import utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using LSTM network.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train-processed-freqdist-bi.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train-processed-freqdist.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/train_stanford_ds-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/test_stanford_ds-processed.csv'\n",
        "GLOVE_FILE = '/content/drive/My Drive/adm_project_dataset/dataset/glove.twitter.27B.200d.txt'\n",
        "dim = 200\n",
        "\n",
        "def write_status(i, total):\n",
        "    ''' Writes status of a process to console '''\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print('Looking for GLOVE vectors')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print('\\n')\n",
        "    print('Found %d words in GLOVE' % found)\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 128\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(64))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"/content/drive/My Drive/models/lstm.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        print(model.summary())\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print(model.summary())\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'lstm.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 134915/0Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}